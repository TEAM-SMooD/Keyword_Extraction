{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=pd.read_csv(r'C:\\Users\\user\\OneDrive\\바탕화~1-DESKTOP-R00ORLS-348\\캡스톤\\data_git\\data\\diningcode_data\\다이닝코드 북촌 리뷰 분리.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>상가업소번호</th>\n",
       "      <th>가게명</th>\n",
       "      <th>원래이름</th>\n",
       "      <th>리뷰</th>\n",
       "      <th>분리리뷰</th>\n",
       "      <th>리뷰날짜</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>칼만두국을 먹었는데 사골베이스에 조금 슴슴한 편인데 맛이 꽤 훌륭하다.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>면발은 하늘하늘 부드러운 편이다.</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...</td>\n",
       "      <td>테이블 간격이 좁고 서버 이모님들이 살짝 불친절한 편이라 온전히 유명세의 맛경험을 ...</td>\n",
       "      <td>2023-3-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20592276</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>깡통만두</td>\n",
       "      <td>주말에 방문하면 웨이팅이 깁니다. 한시간정도 대기하면 드실 수 있어요. 만두가 깔끔...</td>\n",
       "      <td>주말에 방문하면 웨이팅이 깁니다.</td>\n",
       "      <td>2023-2-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     상가업소번호   가게명  원래이름                                                 리뷰  \\\n",
       "0  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "1  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "2  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "3  20592276  깡통만두  깡통만두  만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.칼만두국을 먹었는데 사골베이스에 ...   \n",
       "4  20592276  깡통만두  깡통만두  주말에 방문하면 웨이팅이 깁니다. 한시간정도 대기하면 드실 수 있어요. 만두가 깔끔...   \n",
       "\n",
       "                                                분리리뷰       리뷰날짜  \n",
       "0                       만두가 옛날 집에서 빚은 딱 그 맛이어서, 좋았다.   2023-3-9  \n",
       "1            칼만두국을 먹었는데 사골베이스에 조금 슴슴한 편인데 맛이 꽤 훌륭하다.   2023-3-9  \n",
       "2                                 면발은 하늘하늘 부드러운 편이다.   2023-3-9  \n",
       "3  테이블 간격이 좁고 서버 이모님들이 살짝 불친절한 편이라 온전히 유명세의 맛경험을 ...   2023-3-9  \n",
       "4                                 주말에 방문하면 웨이팅이 깁니다.  2023-2-19  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3142 entries, 0 to 3141\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   상가업소번호  3142 non-null   int64 \n",
      " 1   가게명     3142 non-null   object\n",
      " 2   원래이름    3142 non-null   object\n",
      " 3   리뷰      3142 non-null   object\n",
      " 4   분리리뷰    3142 non-null   object\n",
      " 5   리뷰날짜    3142 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 147.4+ KB\n"
     ]
    }
   ],
   "source": [
    "b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "음.\n",
      "2297\n",
      "꼭\n",
      "2679\n",
      "추천\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(b)):\n",
    "    if len(b['분리리뷰'][i])<3:\n",
    "        print(i)\n",
    "        print(b['분리리뷰'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.drop([1082,1550,1784,1904,3139],axis=0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=b.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review 컬럼만 리스트로 저장\n",
    "text=''\n",
    "reviews=[]\n",
    "for each_review in b['분리리뷰']:\n",
    "    reviews.append(each_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3137"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "import re\n",
    "def clean_str(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    return text   \n",
    "\n",
    "review=[]\n",
    "for i in reviews:  #문자가 들어있을때는 인덱스 사용하면 안됨!!!!\n",
    "    a=clean_str(i)\n",
    "    review.append(a)  #불용어제거한 review 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['만두가 옛날 집에서 빚은 딱 그 맛이어서 좋았다',\n",
       " '칼만두국을 먹었는데 사골베이스에 조금 슴슴한 편인데 맛이 꽤 훌륭하다',\n",
       " '면발은 하늘하늘 부드러운 편이다',\n",
       " '테이블 간격이 좁고 서버 이모님들이 살짝 불친절한 편이라 온전히 유명세의 맛경험을 하기엔 다소 아쉬움이 있다',\n",
       " '주말에 방문하면 웨이팅이 깁니다',\n",
       " '한시간정도 대기하면 드실 수 있어요',\n",
       " '만두가 깔끔하고 맛있네요',\n",
       " '테이블 간 간격이 좁아서 후다닥 먹고 나왔네요 ',\n",
       " '직접 빚은 만두와 직접 뽑은 생면이 있는 안국역 수요미식회 맛집 깡통만두',\n",
       " '만두전골을 시키면 고기만두 해물만두 두 종류를 주는데 맛있고 해물만두는 새우한마리가 통으로 들어있다']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(review)):\n",
    "    if review[i]=='':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'흠'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[347]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 추출 n_gram_range(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 키워드 길이 지정\n",
    "n_gram_range = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m348\u001b[39m,a): \u001b[39m# 리뷰하나당 처리하기위해 for문 \u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m----> 8\u001b[0m     review_vectorized \u001b[39m=\u001b[39m TfidfVectorizer(ngram_range\u001b[39m=\u001b[39;49mn_gram_range)\u001b[39m.\u001b[39;49mfit([review[i]])\n\u001b[0;32m      9\u001b[0m     token_review \u001b[39m=\u001b[39m review_vectorized\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     11\u001b[0m     token_review_list\u001b[39m.\u001b[39mappend(token_review) \u001b[39m# 하나의 리스트를 만들어서 df에 추가해야함.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2103\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2098\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2099\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2100\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2101\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2102\u001b[0m )\n\u001b[1;32m-> 2103\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#리뷰 하나씩 키워드로 분리(TFidf=Vectorizer)\n",
    "\n",
    "a= len(review) # 불용어 제거한 리뷰\n",
    "token_review_list=[]\n",
    "\n",
    "for i in range(348,a): # 리뷰하나당 처리하기위해 for문 \n",
    "    print(i)\n",
    "    review_vectorized = TfidfVectorizer(ngram_range=n_gram_range).fit([review[i]])\n",
    "    token_review = review_vectorized.get_feature_names_out()\n",
    "    \n",
    "    token_review_list.append(token_review) # 하나의 리스트를 만들어서 df에 추가해야함.\n",
    "b['review_tfidf']=token_review_list # 토큰 단위로 나누어진 리뷰저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m token_review_list\u001b[39m=\u001b[39m[]\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,a): \u001b[39m# 리뷰하나당 처리하기위해 for문 \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     review_vectorized \u001b[39m=\u001b[39m CountVectorizer(ngram_range\u001b[39m=\u001b[39;49mn_gram_range)\u001b[39m.\u001b[39;49mfit([review[i]])\n\u001b[0;32m      8\u001b[0m     token_review \u001b[39m=\u001b[39m review_vectorized\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     10\u001b[0m     token_review_list\u001b[39m.\u001b[39mappend(token_review) \u001b[39m# 하나의 리스트를 만들어서 df에 추가해야함.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1339\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, raw_documents, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1324\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m \n\u001b[0;32m   1326\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[39m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1339\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   1340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\capstone\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# 리뷰하나씩 키워드로 분리(count-vectorizer)\n",
    "a= len(review) # 불용어 제거한 리뷰\n",
    "\n",
    "token_review_list=[]\n",
    "\n",
    "for i in range(0,a): # 리뷰하나당 처리하기위해 for문 \n",
    "    review_vectorized = CountVectorizer(ngram_range=n_gram_range).fit([review[i]])\n",
    "    token_review = review_vectorized.get_feature_names_out()\n",
    "    \n",
    "    token_review_list.append(token_review) # 하나의 리스트를 만들어서 df에 추가해야함.\n",
    "\n",
    "b['review_count']=token_review_list # 토큰 단위로 나누어진 리뷰저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
